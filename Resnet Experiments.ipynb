{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datetime\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models import resnet\n",
    "from torchvision.datasets import coco\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = '/Users/peterspradling/CS349D/edge-serving/'\n",
    "m = resnet.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.55s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "ANN_PATH = 'coco/cocoapi/annotations/instances_val2017.json'\n",
    "VAL_PATH = 'coco/cocoapi/images/val2017/'\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "def merge_coco(l):\n",
    "    return torch.stack([x[0] for x in l])\n",
    "c_dataset = coco.CocoDetection(root=BASE+VAL_PATH, annFile=BASE+ANN_PATH, transform=test_transform, )\n",
    "eval_loader = torch.utils.data.DataLoader(c_dataset, batch_size=1, shuffle=True, collate_fn=merge_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mem_size(t):\n",
    "    t_size = t.size()\n",
    "    n_nums = 1\n",
    "    for n in t_size:\n",
    "        n_nums *= n\n",
    "    bit_size_map = {torch.float32 : 32, torch.float16 : 16, torch.float64 : 64, torch.uint8 : 8, torch.int8 : 8, torch.int16 : 16, torch.int32 : 32, torch.int64 : 64}\n",
    "    return (n_nums * bit_size_map[t.dtype]) / 8\n",
    "\n",
    "def resnet_measure_forward(model, x):\n",
    "    print('Image size:', compute_mem_size(x))\n",
    "    t0 = datetime.datetime.now()\n",
    "    x = model.conv1(x)\n",
    "    x = model.bn1(x)\n",
    "    x = model.relu(x)\n",
    "    t1 = datetime.datetime.now()\n",
    "    dt1 = t1 - t0\n",
    "    print('After conv1 (' + str(dt1.total_seconds()) + '):', compute_mem_size(x))\n",
    "    t2 = datetime.datetime.now()\n",
    "    x = model.maxpool(x)\n",
    "    t3 = datetime.datetime.now()\n",
    "    dt2 = t3 - t2\n",
    "    print('After maxpool (' + str(dt2.total_seconds()) + '):', compute_mem_size(x))\n",
    "    t4 = datetime.datetime.now()\n",
    "    x = model.layer1(x)\n",
    "    t5 = datetime.datetime.now()\n",
    "    dt3 = t5 - t4\n",
    "    print('After l1 (' + str(dt3.total_seconds()) + '):', compute_mem_size(x))\n",
    "    t6 = datetime.datetime.now()\n",
    "    x = model.layer2(x)\n",
    "    t7 = datetime.datetime.now()\n",
    "    dt4 = t7 - t6\n",
    "    print('After l2 (' + str(dt4.total_seconds()) + '):', compute_mem_size(x))\n",
    "    t8 = datetime.datetime.now()\n",
    "    x = model.layer3(x)\n",
    "    t9 = datetime.datetime.now()\n",
    "    dt5 = t9 - t8\n",
    "    print('After l3 (' + str(dt5.total_seconds()) + '):', compute_mem_size(x))\n",
    "    t10 = datetime.datetime.now()\n",
    "    x = model.layer4(x)\n",
    "    t11 = datetime.datetime.now()\n",
    "    dt6 = t11 - t10\n",
    "    print('After l4 (' + str(dt6.total_seconds()) + '):', compute_mem_size(x))\n",
    "    t12 = datetime.datetime.now()\n",
    "    x = model.avgpool(x)\n",
    "    t13 = datetime.datetime.now()\n",
    "    dt7 = t13 - t12\n",
    "    print('After avgpool (' + str(dt7.total_seconds()) + '):', compute_mem_size(x))\n",
    "    x = x.view(x.size(0), -1)\n",
    "    t14 = datetime.datetime.now()\n",
    "    x = model.fc(x)\n",
    "    t15 = datetime.datetime.now()\n",
    "    dt8 = t15 - t14\n",
    "    print('After fc (' + str(dt8.total_seconds()) + '):', compute_mem_size(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 602112.0\n",
      "After conv1 (0.009524): 3211264.0\n",
      "After maxpool (0.008569): 802816.0\n",
      "After l1 (0.027385): 802816.0\n",
      "After l2 (0.019474): 401408.0\n",
      "After l3 (0.015558): 200704.0\n",
      "After l4 (0.016637): 100352.0\n",
      "After avgpool (0.000185): 2048.0\n",
      "After fc (0.000293): 4000.0\n"
     ]
    }
   ],
   "source": [
    "m.eval()\n",
    "for batch in eval_loader:\n",
    "    resnet_measure_forward(m, batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 602112.0\n",
      "After conv1 (0.015966): 3211264.0\n",
      "After maxpool (0.012063): 802816.0\n",
      "After l1 (0.032401): 802816.0\n",
      "After l2 (0.026047): 401408.0\n",
      "After l3 (0.015403): 200704.0\n",
      "After l4 (0.018391): 100352.0\n",
      "After avgpool (0.000191): 2048.0\n",
      "After fc (0.000231): 4000.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "pic should be Tensor or ndarray. Got <class 'torch.Tensor'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9f829ef96c4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mresnet_measure_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#transforms.ToPILImage()(sample_input)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.5/site-packages/torchvision-0.2.1-py3.5.egg/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.5/site-packages/torchvision-0.2.1-py3.5.egg/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \"\"\"\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_is_numpy_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_is_tensor_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pic should be Tensor or ndarray. Got {}.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mnpimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: pic should be Tensor or ndarray. Got <class 'torch.Tensor'>."
     ]
    }
   ],
   "source": [
    "\n",
    "#transforms.ToPILImage()(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
