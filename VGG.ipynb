{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datetime\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models import vgg\n",
    "from torchvision.datasets import coco\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = '/Users/ajitpunj/Documents/School/cs 349d/project/'\n",
    "m = vgg.vgg11(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.81s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "ANN_PATH = 'coco/cocoapi/annotations/instances_val2017.json'\n",
    "VAL_PATH = 'coco/cocoapi/images/val2017/'\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224), #also uses 224 cropped images\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "def merge_coco(l):\n",
    "    return torch.stack([x[0] for x in l])\n",
    "c_dataset = coco.CocoDetection(root=BASE+VAL_PATH, annFile=BASE+ANN_PATH, transform=test_transform, )\n",
    "eval_loader = torch.utils.data.DataLoader(c_dataset, batch_size=1, shuffle=True, collate_fn=merge_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mem_size(t):\n",
    "    t_size = t.size()\n",
    "    n_nums = 1\n",
    "    for n in t_size:\n",
    "        n_nums *= n\n",
    "    bit_size_map = {torch.float32 : 32, torch.float16 : 16, torch.float64 : 64, torch.uint8 : 8, torch.int8 : 8, torch.int16 : 16, torch.int32 : 32, torch.int64 : 64}\n",
    "    return (n_nums * bit_size_map[t.dtype]) / 8\n",
    "\n",
    "def vgg_measure_forward(model, x):\n",
    "    print('Image size:', compute_mem_size(x))\n",
    "    for module in model.features:\n",
    "        t1 = datetime.datetime.now()\n",
    "        x = module(x)\n",
    "        t2 = datetime.datetime.now()\n",
    "        dt1 = t2 - t1\n",
    "        canon_name = type(module).__name__\n",
    "        print('After ' + canon_name + '(' + str(dt1.total_seconds()) + '):  ' + str(compute_mem_size(x)))\n",
    "        \n",
    "    #x = model.features(x)\n",
    "    x = x.view(x.size(0), -1)\n",
    "    for module in model.classifier:\n",
    "        t1 = datetime.datetime.now()\n",
    "        x = module(x)\n",
    "        t2 = datetime.datetime.now()\n",
    "        dt1 = t2 - t1\n",
    "        canon_name = type(module).__name__\n",
    "        print('After ' + canon_name + '(' + str(dt1.total_seconds()) + '):  ' + str(compute_mem_size(x)))\n",
    "           \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 602112.0\n",
      "After Conv2d(0.017083):  12845056.0\n",
      "After ReLU(0.004961):  12845056.0\n",
      "After MaxPool2d(0.022291):  3211264.0\n",
      "After Conv2d(0.033365):  6422528.0\n",
      "After ReLU(0.004022):  6422528.0\n",
      "After MaxPool2d(0.009641):  1605632.0\n",
      "After Conv2d(0.01697):  3211264.0\n",
      "After ReLU(0.002421):  3211264.0\n",
      "After Conv2d(0.063252):  3211264.0\n",
      "After ReLU(0.001958):  3211264.0\n",
      "After MaxPool2d(0.005746):  802816.0\n",
      "After Conv2d(0.024533):  1605632.0\n",
      "After ReLU(0.000965):  1605632.0\n",
      "After Conv2d(0.04537):  1605632.0\n",
      "After ReLU(0.000494):  1605632.0\n",
      "After MaxPool2d(0.001965):  401408.0\n",
      "After Conv2d(0.010566):  401408.0\n",
      "After ReLU(0.000269):  401408.0\n",
      "After Conv2d(0.010341):  401408.0\n",
      "After ReLU(0.000196):  401408.0\n",
      "After MaxPool2d(0.00044):  100352.0\n",
      "After Linear(0.024541):  16384.0\n",
      "After ReLU(0.000174):  16384.0\n",
      "After Dropout(0.000118):  16384.0\n",
      "After Linear(0.004842):  16384.0\n",
      "After ReLU(0.000193):  16384.0\n",
      "After Dropout(4.8e-05):  16384.0\n",
      "After Linear(0.001224):  4000.0\n"
     ]
    }
   ],
   "source": [
    "m.eval()\n",
    "for batch in eval_loader:\n",
    "    vgg_measure_forward(m, batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ad44f006cc67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_input' is not defined"
     ]
    }
   ],
   "source": [
    "transforms.ToPILImage()(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
